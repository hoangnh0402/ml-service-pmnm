# ML Service - IoT Data Classification

High-performance FastAPI microservice for classifying IoT sensor data with optimized low-latency predictions.

## üöÄ Features

- **Ultra-low latency**: Rule-based classification with sub-millisecond prediction time
- **High throughput**: Async FastAPI with multiple Uvicorn workers
- **Production-ready**: Docker support with multi-stage builds and health checks
- **Auto-documentation**: Interactive API docs at `/docs`
- **CORS enabled**: Ready for backend integration
- **Performance monitoring**: Each response includes processing time metrics

## üìä Classification Logic

The service classifies sensor data into three categories based on temperature and CO2 levels:

| Label | Conditions | Description |
|-------|-----------|-------------|
| **HOT** | `temperature > 50` OR `co2_level > 1000` | Critical: Fire/gas hazard warning |
| **WARM** | `temperature > 35` | Elevated temperature, not dangerous |
| **COLD** | All other cases | Normal conditions |

## üõ†Ô∏è Tech Stack

- **Python**: 3.9+
- **Framework**: FastAPI 0.104.1
- **Server**: Uvicorn with standard optimizations
- **Validation**: Pydantic v2
- **Libraries**: scikit-learn, pandas, numpy (for future ML integration)

## üì° API Endpoints

### POST /predict

Classify IoT sensor data.

**Request Body:**
```json
{
  "temperature": 45.5,
  "humidity": 65,
  "co2_level": 850
}
```

**Response:**
```json
{
  "label": "WARM",
  "confidence": 1.0,
  "temperature": 45.5,
  "co2_level": 850,
  "processing_time_ms": 0.5
}
```

**Example with curl:**
```bash
curl -X POST "http://localhost:5000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "temperature": 55.0,
    "humidity": 70,
    "co2_level": 900
  }'
```

### GET /health

Health check endpoint for monitoring.

**Response:**
```json
{
  "status": "healthy",
  "service": "ml-service-pmnm",
  "timestamp": 1701234567.89
}
```

### GET /

Service information and available endpoints.

### GET /docs

Interactive Swagger UI documentation.

## üèÉ Quick Start

### Local Development

1. **Install dependencies:**
```bash
pip install -r requirements.txt
```

2. **Run the service:**
```bash
uvicorn main:app --host 0.0.0.0 --port 5000 --reload
```

3. **Access the API:**
- API: http://localhost:5000
- Docs: http://localhost:5000/docs
- Health: http://localhost:5000/health

### Docker Deployment (Recommended)

#### Option 1: Docker Compose
```bash
# Start the service
docker-compose up -d

# View logs
docker-compose logs -f ml-service

# Stop the service
docker-compose down
```

#### Option 2: Manual Docker Build
```bash
# Build the image
docker build -t ml-service-pmnm .

# Run the container
docker run -d -p 5000:5000 --name ml-service ml-service-pmnm

# Check status
docker ps
docker logs ml-service
```

## üìù Example Usage Scenarios

### Normal conditions (COLD)
```bash
curl -X POST "http://localhost:5000/predict" \
  -H "Content-Type: application/json" \
  -d '{"temperature": 25.0, "humidity": 50, "co2_level": 400}'
# Expected: {"label": "COLD", ...}
```

### Elevated temperature (WARM)
```bash
curl -X POST "http://localhost:5000/predict" \
  -H "Content-Type: application/json" \
  -d '{"temperature": 40.0, "humidity": 60, "co2_level": 500}'
# Expected: {"label": "WARM", ...}
```

### Critical conditions (HOT)
```bash
curl -X POST "http://localhost:5000/predict" \
  -H "Content-Type: application/json" \
  -d '{"temperature": 55.0, "humidity": 70, "co2_level": 1200}'
# Expected: {"label": "HOT", ...}
```

## ‚ö° Performance Optimization

The service is optimized for low latency:

1. **Rule-based classification**: No ML model loading overhead
2. **In-memory processing**: Zero disk I/O during predictions
3. **Async endpoints**: Non-blocking request handling
4. **Multiple workers**: 2 Uvicorn workers for concurrent requests
5. **Minimal dependencies**: Only essential packages included
6. **Multi-stage Docker build**: Smaller image size, faster container startup

### Performance Characteristics

Based on the implementation:
- **Classification time**: < 1ms (rule-based, in-memory)
- **Request processing**: < 10ms end-to-end
- **Expected throughput**: 1000+ requests/second
- **Memory footprint**: ~50MB (minimal container)
- **Startup time**: < 5 seconds

## üèóÔ∏è Architecture Overview

### Core Components

#### FastAPI Application (`main.py`)
- **Rule-based classification logic**: Optimized for sub-millisecond predictions
- **Pydantic models**: `SensorData` for input validation, `PredictionResponse` for output
- **REST API endpoints**: `/predict`, `/health`, `/`, `/docs`
- **CORS middleware**: Enabled for backend integration
- **Performance tracking**: Each response includes `processing_time_ms`

#### Docker Configuration
- **Multi-stage Dockerfile**: Optimized for production with minimal image size
- **Docker Compose**: Easy deployment with health checks and networking
- **Security**: Non-root user (appuser) for container execution
- **Health checks**: Built-in monitoring for orchestration platforms

#### Dependencies (`requirements.txt`)
Pinned versions for reproducibility:
- `fastapi==0.104.1` - Modern async web framework
- `uvicorn[standard]==0.24.0` - High-performance ASGI server
- `pydantic==2.5.0` - Data validation
- `scikit-learn==1.3.2` - For future ML model integration
- `pandas==2.1.3` - Data processing
- `numpy==1.26.2` - Numerical operations

## üîß Configuration

### Environment Variables

- `PYTHONUNBUFFERED=1`: Disable Python output buffering
- `LOG_LEVEL`: Set logging level (default: info)

### Uvicorn Settings

Modify in `Dockerfile` or when running locally:
- `--workers`: Number of worker processes (default: 2)
- `--host`: Bind host (default: 0.0.0.0)
- `--port`: Bind port (default: 5000)

## üö¶ Health Monitoring

The service includes health checks for container orchestration:

- **Endpoint**: `/health`
- **Interval**: 30 seconds
- **Timeout**: 10 seconds
- **Retries**: 3

Use with Kubernetes, Docker Swarm, or any orchestration platform.

## üöÄ Deployment Recommendations

### For Development
- Use `uvicorn` with `--reload` flag
- Access interactive docs at `/docs`
- Monitor `processing_time_ms` in responses

### For Production
- Deploy using Docker or docker-compose
- Use environment variables for configuration
- Implement API rate limiting (future enhancement)
- Set up logging aggregation
- Configure health check intervals based on load

### For Integration with Backend
- The service exposes port **5000** as requested
- CORS is enabled for all origins (configure appropriately for production)
- Use `/predict` endpoint for classification
- Use `/health` for monitoring

## ‚ú® Implementation Highlights

1. **Optimized classification function** (`classify_sensor_data`):
   - Pure Python logic, no external calls
   - Simple if-elif-else for minimal overhead
   - Returns deterministic results (confidence always 1.0)

2. **Async FastAPI endpoints**:
   - Non-blocking request handling
   - Concurrent request support
   - Built-in request validation via Pydantic

3. **Multi-stage Docker build**:
   - Separate builder and runtime stages
   - Virtual environment isolation
   - Minimal runtime dependencies

4. **Comprehensive error handling**:
   - Pydantic validates input automatically
   - Try-catch in predict endpoint
   - Clear HTTP error responses

## üì¶ Project Structure

```
ml-service-pmnm/
‚îú‚îÄ‚îÄ main.py                 # FastAPI application
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îú‚îÄ‚îÄ Dockerfile             # Multi-stage container build
‚îú‚îÄ‚îÄ docker-compose.yml     # Easy deployment config
‚îú‚îÄ‚îÄ .dockerignore          # Docker build optimization
‚îú‚îÄ‚îÄ .gitignore             # Git exclusions
‚îî‚îÄ‚îÄ README.md              # Comprehensive documentation
```

## üîÆ Future Enhancements

- [ ] Add real ML model support (scikit-learn/TensorFlow)
- [ ] Model versioning and A/B testing
- [ ] Prometheus metrics export
- [ ] Request rate limiting
- [ ] Authentication/API keys
- [ ] Batch prediction endpoint
- [ ] Model performance monitoring
- [ ] Caching for frequent queries

## üß™ Testing Guide

### Automated Testing

You can test all three classification scenarios:

**Test 1: COLD (Normal conditions)**
```bash
curl -X POST "http://localhost:5000/predict" -H "Content-Type: application/json" -d '{"temperature": 25.0, "humidity": 50, "co2_level": 400}'
```

**Test 2: WARM (Elevated temperature)**
```bash
curl -X POST "http://localhost:5000/predict" -H "Content-Type: application/json" -d '{"temperature": 40.0, "humidity": 60, "co2_level": 500}'
```

**Test 3: HOT (Critical conditions - high temp)**
```bash
curl -X POST "http://localhost:5000/predict" -H "Content-Type: application/json" -d '{"temperature": 55.0, "humidity": 70, "co2_level": 800}'
```

**Test 4: HOT (Critical conditions - high CO2)**
```bash
curl -X POST "http://localhost:5000/predict" -H "Content-Type: application/json" -d '{"temperature": 30.0, "humidity": 65, "co2_level": 1200}'
```

**Test 5: Health Check**
```bash
curl http://localhost:5000/health
```

### Interactive Testing

Open http://localhost:5000/docs in your browser to use the Swagger UI for interactive testing.

## üéì Next Steps

The service is ready for integration with your backend! Here's what you can do:

1. **Test locally** using the curl examples or interactive docs
2. **Deploy with Docker** for consistent environment
3. **Integrate with backend** - the service is already CORS-enabled
4. **Monitor performance** - check `processing_time_ms` in responses
5. **Extend functionality** - add real ML models when needed

The codebase is well-structured for future enhancements like loading actual ML models, adding batch prediction endpoints, and implementing advanced monitoring.

## üìÑ License

This project is for the Smart City Platform (OLP 2025).

## üë• Support

For issues or questions, please contact the development team.

---

**Built with ‚ù§Ô∏è using FastAPI**
